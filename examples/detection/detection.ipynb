{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Detection Model Execution using DeepSparse Engine\n",
    "\n",
    "This notebook provides an example of how to perform inference on a pretrained, dense object detection model using the DeepSparse Engine, with the model and sample data provided by SparseZoo.\n",
    "\n",
    "You will:\n",
    "- Download your exported pretrained ONNX model of choice and sample data from SparseZoo\n",
    "- Compile a CPU-optimized executable of that model with the DeepSparse Engine\n",
    "- Run that model with real data\n",
    "- Benchmark the speed of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "To run this notebook you will need to have installed:\n",
    "- DeepSparse Engine and SparseZoo\n",
    "- Matplotlib for visualization of the results\n",
    "\n",
    "Feel free to install any package you require using `pip`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deepsparse\n",
    "import sparsezoo\n",
    "\n",
    "import json\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gathering the Model and Data\n",
    "\n",
    "By default, you will download a YOLOv3 model trained on the COCO dataset.\n",
    "The model's pretrained weights and exported ONNX file are downloaded from the SparseZoo model repo.\n",
    "The sample batch of data is downloaded from SparseZoo as well.\n",
    "\n",
    "If you want to try different architectures replace `yolo_v3()` with your choice, for example: `ssd_resnet50_300()`.\n",
    "\n",
    "You may also want to try different batch sizes to evaluate accuracy and performance for your task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inspect import getmembers, isfunction\n",
    "from sparsezoo.models import detection\n",
    "\n",
    "print(\n",
    "    \"Available default detection models:\",\n",
    "    *dict(getmembers(detection, isfunction)).keys()\n",
    ")\n",
    "\n",
    "# =====================================================\n",
    "# Define your batch size for inference below\n",
    "# =====================================================\n",
    "batch_size = 16\n",
    "\n",
    "# =====================================================\n",
    "# Define your model below\n",
    "# =====================================================\n",
    "print(\"Downloading model...\")\n",
    "model = detection.ssd_resnet50_300()\n",
    "\n",
    "# Gather sample batch of data for inference and visualization\n",
    "batch = model.sample_batch(batch_size=batch_size)\n",
    "batched_inputs = batch[\"inputs\"]\n",
    "batched_outputs = batch[\"outputs\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile Model for Inference\n",
    "\n",
    "The DeepSparse Engine will compile your model into an optimized executable for inference for a given batch size, making use of natural sparsity that arises from traditional deep learning flows.\n",
    "\n",
    "By default, it will make use of all physical cores on your system. Feel free to adjust the `num_cores` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepsparse import compile_model, cpu\n",
    "\n",
    "CORES_PER_SOCKET, AVX_TYPE, _ = cpu.cpu_details()\n",
    "\n",
    "# =====================================================\n",
    "# Define the number of cores to use\n",
    "# =====================================================\n",
    "num_cores = CORES_PER_SOCKET\n",
    "\n",
    "print(\"Compiling {} model with DeepSparse Engine\".format(model.architecture_id))\n",
    "engine = compile_model(model.onnx_file.downloaded_path(), batch_size, num_cores)\n",
    "print(engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "Now that the model has an engine compiled for it, we can feed it inputs and get predicted outputs.\n",
    "\n",
    "Using SparseZoo, we can compare the predicted output against the ground-truth classes to validate the model is still accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from deepsparse.utils import verify_outputs\n",
    "\n",
    "# Record output from inference through the DeepSparse Engine\n",
    "print(\"Executing...\")\n",
    "predicted_outputs = engine(batched_inputs)\n",
    "\n",
    "# Compare against reference model output\n",
    "print(\"Validating against reference outputs...\")\n",
    "verify_outputs(predicted_outputs, batched_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking\n",
    "\n",
    "The DeepSparse Engine exposes simple interfaces to benchmark its inference speeds over a variety of situations. \n",
    "\n",
    "The result of these benchmarks can be viewed in intuitive ways that help you understand where you can deploy the Engine, with measurements like `items_per_second` readily available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_result = engine.benchmark(\n",
    "    batched_inputs, num_iterations=50, num_warmup_iterations=10\n",
    ")\n",
    "print(benchmark_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Data\n",
    "\n",
    "To further visualize the inputs of your inference, you can view the input data using Matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Plot each input image from model inference\n",
    "nrows = ncols = int(numpy.sqrt(batch_size))\n",
    "fig, ax = plt.subplots(nrows, ncols, figsize=(3 * ncols, 3 * nrows))\n",
    "i = 0\n",
    "for row in ax:\n",
    "    for col in row:\n",
    "        image = batched_inputs[0][i].copy()\n",
    "        pixels = numpy.transpose(image, (1, 2, 0))\n",
    "        pixels = numpy.interp(pixels, (pixels.min(), pixels.max()), (0.0, 1.0))\n",
    "\n",
    "        # Draw image and predicted label\n",
    "        col.imshow(pixels)\n",
    "\n",
    "        i += 1\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Great job - you have downloaded a model from SparseZoo, used the DeepSparse Engine for inference, and validated the output!\n",
    "\n",
    "Next steps to pursue include:\n",
    "\n",
    "- Try other object detection models from SparseZoo\n",
    "    - Pruned versions of the model to see unstructured sparsity speedup on the DeepSparse Engine\n",
    "    - Other architectures: SSD, YOLO\n",
    "- Use benchmarking scripts to compare performance across models and inference engines"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
