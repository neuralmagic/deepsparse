{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<sub>&copy; 2021-present Neuralmagic, Inc. // [Neural Magic Legal](https://neuralmagic.com/legal)</sub> \n",
    "\n",
    "# Running YOLO Models on the DeepSparse Engine\n",
    "\n",
    "In [this blog post](https://neuralmagic.com/blog/benchmark-yolov3-on-cpus-with-deepsparse/) the Neural Magic team describes how achieving real time inference speeds on [Ultralytic's YOLOv3](https://github.com/ultralytics/yolov5) using CPUs is possible. This notebook provides a step-by-step walkthrough for using the [DeepSparse](https://github.com/neuralmagic/deepsparse) inference engine to attain and benchmark these speeds.\n",
    "\n",
    "This notebook runs pre-sparsified models downloaded from the [SparseZoo](https://github.com/neuralmagic/sparsezoo). For information on creating custom sparsified YOLO models with [SparseML](https://github.com/neuralmagic/sparseml), check out the [SparseML-Ultralytics integration documentation](https://github.com/neuralmagic/sparseml/blob/main/integrations/ultralytics/README.md).\n",
    "\n",
    "\n",
    "In this notebook, you will:\n",
    "- Compile a model with the DeepSparse engine\n",
    "- Use the DeepSparse engine to run a sample inferences with YOLOv3 and annotate the output\n",
    "- Run benchmarks of YOLOv3 on the DeepSparse engine\n",
    "\n",
    "## Setting Up\n",
    "This notebook depends on other files from SparseML's Ultralytics integration examples as well as Ultralytic's yolov5 repo.  Run the following to start this notebook, and make sure this notebook is in the yolov5 directory.\n",
    "\n",
    "```bash\n",
    "# clone\n",
    "git clone https://github.com/ultralytics/yolov5.git\n",
    "git clone https://github.com/neuralmagic/sparseml.git\n",
    "\n",
    "# copy example files\n",
    "cd yolov5\n",
    "cp ../sparseml/integrations/ultralytics/deepsparse/* .\n",
    "\n",
    "# install dependencies\n",
    "pip install sparseml[torchvision] deepsparse\n",
    "pip install -r requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SparseZoo Stubs\n",
    "[SparseZoo](https://github.com/neuralmagic/sparsezoo) stubs refer to models and SparseML recipes that are stored in the SparseZoo. They can be used to easily access these files through the `sparsezoo.Zoo` helper class, or even to load these models directly into DeepSparse.\n",
    "\n",
    "The following cell defines SparseZoo stubs for 3 different models:\n",
    "* *base*: YOLOv3-SPP baseline dense\n",
    "* *pruned*: YOLOv3-SPP pruned to 88% sparsity\n",
    "* *pruned_quant*: YOLOv3-SPP pruned to 83% sparsity and quantized with INT8 quantization\n",
    "\n",
    "These stubs can be easily used for inference and benchmarking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_STUB = \"zoo:cv/detection/yolo_v3-spp/pytorch/ultralytics/coco/base-none\"\n",
    "PRUNED_STUB = \"zoo:cv/detection/yolo_v3-spp/pytorch/ultralytics/coco/pruned-aggressive_97\"\n",
    "PRUNED_QUANT_STUB = \"zoo:cv/detection/yolo_v3-spp/pytorch/ultralytics/coco/pruned_quant-aggressive_94\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "In the following cells, you will download some sample images from the SparseZoo, compile a model with the DeepSparse engine, and use this model to perform a sample inference.\n",
    "\n",
    "The basic flow for using the DeepSparse engine for inference is just two steps: compiling a model and running a forward pass with the model:\n",
    "\n",
    "```python\n",
    "from deepsparse import compile_model\n",
    "import numpy\n",
    "\n",
    "# compile\n",
    "model = compile_model(MODEL_PATH, batch_size=BATCH_SIZE)\n",
    "\n",
    "# inference\n",
    "outputs = model([numpy.random.randn(1, 3, 640, 640)])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Sample Images\n",
    "The following cell uses a helper function to download sample images from the SparseZoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from benchmark import load_images\n",
    "\n",
    "images, original_images = load_images(None, (640, 640))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile Model\n",
    "The following cell compiles a DeepSparse model from the YOLOv3 pruned SparseZoo stub.  To use a different YOLO model, replace `sample_model_path` with a different stub path or the path to a local ONNX model file. This example does require that the model expects 640,640 sized image inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepsparse import compile_model\n",
    "\n",
    "sample_model_path = PRUNED_STUB  # replace with file path or stub of choice\n",
    "model = compile_model(sample_model_path, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference\n",
    "The following cell runs pre-processing, inference, post-processing, and NMS on a sample image.  Try changing the `SAMPLE_IMAGE_IDX` (can be 0-19) to see different inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from deepsparse_utils import YoloPostprocessor, postprocess_nms\n",
    "\n",
    "SAMPLE_IMAGE_IDX = 0\n",
    "\n",
    "sample_image = images[SAMPLE_IMAGE_IDX]\n",
    "sample_image = sample_image.reshape(1, *sample_image.shape)\n",
    "if sample_model_path != PRUNED_QUANT_STUB:  # preprocess images for non-quantized\n",
    "    sample_image = sample_image.astype(numpy.float32) / 255.0\n",
    "sample_batch = [numpy.ascontiguousarray(sample_image)]\n",
    "\n",
    "outputs = model(sample_batch)\n",
    "outputs = YoloPostprocessor().pre_nms_postprocess(outputs)\n",
    "outputs = postprocess_nms(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annotate\n",
    "The following cell visualizes the output from DeepSparse by annotating the original image with the returned bounding boxes and classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from deepsparse_utils import annotate_image\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "annotated_image = annotate_image(original_images[SAMPLE_IMAGE_IDX], outputs[0], {})\n",
    "plt.imshow(annotated_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark\n",
    "The following cells run benchmarks of the base, pruned, and pruned-quantized models.  Benchmark times are end-to-end and include pre-processing, inference, post-processing and NMS.  The defualt setting is a \"real-time\" batch-size 1 example with the maximum number of CPU cores available used. To try benchmarking other deployment scenarios, edit the `BATCH_SIZE`, `NUM_CORES`, and `IMAGE_SIZE` variables.\n",
    "\n",
    "The benchmarking function provided below calls into the `benchmark.py` script that is also provided in this examples directory.  For best results, that script should be used as processes in the jupyter notebook may interfere with the benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from benchmark import benchmark_yolo, parse_args\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "NUM_CORES = None  # set to integer to use\n",
    "IMAGE_SIZE = [\"640\", \"640\"]\n",
    "\n",
    "os.environ[\"NM_BIND_THREADS_TO_CORES\"] = \"1\"  # bind threads to cores\n",
    "\n",
    "\n",
    "def benchmark(model_path, quantized=False):\n",
    "    benchmark_args = [\n",
    "        model_path,\n",
    "        f\"--batch-size={BATCH_SIZE}\",\n",
    "        \"--image-shape\",\n",
    "        *IMAGE_SIZE,\n",
    "    ]\n",
    "    if NUM_CORES:\n",
    "        benchmark_args.append(\"--num-cores={NUM_CORES}\")\n",
    "    if quantized:\n",
    "        benchmark_args.append(\"--quantized-inputs\")\n",
    "    benchmark_args = parse_args(benchmark_args)\n",
    "    benchmark_yolo(benchmark_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark(BASE_STUB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pruned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark(PRUNED_STUB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pruned-Quantized Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark(PRUNED_QUANT_STUB, quantized=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Congratulations, you have created run a sample inference of YOLOv3 using the DeepSparse engine and benchmarked its performance across various deployment settings. Next steps can include:\n",
    "* Sparsify other models, or this model for other datasets using the SparseML-Ultralytics integration\n",
    "* Try the other exmaples in this directory\n",
    "* Learn more about [Neural Magic](https://github.com/neuralmagic) and [Ultralytic's](https://github.com/ultralytics) open sourced repositories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
