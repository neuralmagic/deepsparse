{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Model Execution using DeepSparse Engine\n",
    "\n",
    "This notebook provides an example of how to perform inference on a pretrained, dense classification model using the DeepSparse Engine, with the model and sample data provided by SparseZoo.\n",
    "\n",
    "You will:\n",
    "- Download your exported pretrained ONNX model of choice and sample data from SparseZoo\n",
    "- Compile a CPU-optimized executable of that model with the DeepSparse Engine\n",
    "- Run that model with real data\n",
    "- Verify the accuracy of the model\n",
    "- Benchmark the speed of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "To run this notebook you will need to have installed:\n",
    "- DeepSparse Engine and SparseZoo\n",
    "- Matplotlib for visualization of the results\n",
    "\n",
    "Feel free to install any package you require using `pip`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deepsparse\n",
    "import sparsezoo\n",
    "\n",
    "import json\n",
    "import numpy\n",
    "\n",
    "def calculate_top1_accuracy(pred: numpy.array, labels: numpy.array):\n",
    "    \"\"\"\n",
    "    :param pred: the models prediction to compare with\n",
    "    :param lab: the labels for the data to compare to\n",
    "    :return: the calculated top1 accuracy\n",
    "    \"\"\"\n",
    "    batch_size = pred.shape[0]\n",
    "    pred = numpy.argmax(pred, axis=-1)\n",
    "    labels = numpy.argmax(labels, axis=-1)\n",
    "    \n",
    "    correct = (pred == labels).sum()\n",
    "    correct *= (100.0 / batch_size)\n",
    "    \n",
    "    return correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gathering the Model and Data\n",
    "\n",
    "By default, you will download a MobileNetV1 model trained on the ImageNet dataset.\n",
    "The model's pretrained weights and exported ONNX file are downloaded from the SparseZoo model repo.\n",
    "The sample batch of data is downloaded from SparseZoo as well.\n",
    "\n",
    "If you want to try different architectures replace `mobilenet_v1()` with your choice, for example: `resnet50()` or `efficientnet_b0()`.\n",
    "\n",
    "You may also want to try different batch sizes to evaluate accuracy and performance for your task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inspect import getmembers, isfunction\n",
    "from sparsezoo.models import classification\n",
    "\n",
    "print(\n",
    "    \"Available default classification models:\",\n",
    "    dict(getmembers(classification, isfunction)).keys(),\n",
    ")\n",
    "\n",
    "# =====================================================\n",
    "# Define your batch size for inference below\n",
    "# =====================================================\n",
    "batch_size = 16\n",
    "\n",
    "# =====================================================\n",
    "# Define your model below\n",
    "# =====================================================\n",
    "print(\"Downloading model...\")\n",
    "model = classification.mobilenet_v1()\n",
    "\n",
    "# Gather sample batch of data for inference and visualization\n",
    "batch = model.sample_batch(batch_size=batch_size)\n",
    "batched_inputs = batch[\"inputs\"]\n",
    "batched_outputs = batch[\"outputs\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile Model for Inference\n",
    "\n",
    "The DeepSparse Engine will compile your model into an optimized executable for inference for a given batch size, making use of natural sparsity that arises from traditional deep learning flows.\n",
    "\n",
    "By default, it will make use of all physical cores on your system. Feel free to adjust the `num_cores` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepsparse import compile_model, cpu\n",
    "\n",
    "CORES_PER_SOCKET, AVX_TYPE, _ = cpu.cpu_details()\n",
    "\n",
    "# =====================================================\n",
    "# Define the number of cores to use\n",
    "# =====================================================\n",
    "num_cores = CORES_PER_SOCKET\n",
    "\n",
    "print(\"Compiling {} model with DeepSparse Engine\".format(model.architecture_id))\n",
    "engine = compile_model(model.onnx_file.downloaded_path(), batch_size, num_cores)\n",
    "print(engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "Now that the model has an engine compiled for it, we can feed it inputs and get predicted outputs.\n",
    "\n",
    "Using SparseZoo, we can compare the predicted output against the ground-truth classes to validate the model is still accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from deepsparse.utils import verify_outputs\n",
    "\n",
    "# Record output from inference through the DeepSparse Engine\n",
    "print(\"Executing...\")\n",
    "predicted_outputs = engine(batched_inputs)\n",
    "\n",
    "# Compare against reference model output\n",
    "print(\"Validating against reference outputs...\")\n",
    "verify_outputs(predicted_outputs, batched_outputs)\n",
    "\n",
    "# Measure accuracy against ground truth labels\n",
    "predicted_labels = numpy.argmax(predicted_outputs[-1], axis=-1)\n",
    "if \"labels\" in batch:\n",
    "    batched_labels = batch[\"labels\"]\n",
    "    accuracy = calculate_top1_accuracy(predicted_outputs[-1], batched_labels[0])\n",
    "    print(\"Top-1 Accuracy for batch size {}: {:.2f}%\".format(batch_size, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking\n",
    "\n",
    "The DeepSparse Engine exposes simple interfaces to benchmark its inference speeds over a variety of situations. \n",
    "\n",
    "The result of these benchmarks can be viewed in intuitive ways that help you understand where you can deploy the Engine, with measurements like `items_per_second` readily available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_result = engine.benchmark(\n",
    "    batched_inputs, num_iterations=50, num_warmup_iterations=10\n",
    ")\n",
    "print(benchmark_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Results\n",
    "\n",
    "To further visualize the results of your inference, you can view the input data as the original images with their predicted labels using Matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import json\n",
    "\n",
    "# Load in ImageNet transform information and label mappings\n",
    "with open(\"imagenet.json\") as f:\n",
    "    imagenet_data = json.load(f)\n",
    "\n",
    "\n",
    "# Plot each input image with predicted label from model inference\n",
    "nrows = ncols = int(numpy.sqrt(batch_size))\n",
    "fig, ax = plt.subplots(nrows, ncols, figsize=(3 * ncols, 3 * nrows))\n",
    "i = 0\n",
    "for row in ax:\n",
    "    for col in row:\n",
    "        label = predicted_labels[i]\n",
    "        image = batched_inputs[0][i].copy()\n",
    "\n",
    "        # Transform input tensor back to original values and structure\n",
    "        pixels = numpy.transpose(image, (1, 2, 0))\n",
    "        pixels = (\n",
    "            pixels * imagenet_data[\"transforms\"][\"normalize-std\"]\n",
    "        ) + imagenet_data[\"transforms\"][\"normalize-mean\"]\n",
    "        pixels = numpy.clip(pixels, 0.0, 1.0)\n",
    "\n",
    "        # Draw image and predicted label\n",
    "        col.imshow(pixels)\n",
    "        col.set_title(\"{}\".format(imagenet_data[\"label_mappings\"][str(label)]))\n",
    "\n",
    "        i += 1\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Great job - you have downloaded a model from SparseZoo, used the DeepSparse Engine for inference, and validated the output!\n",
    "\n",
    "Next steps to pursue include:\n",
    "\n",
    "- Try other classification models from SparseZoo\n",
    "    - Pruned versions of the model to see unstructured sparsity speedup on the DeepSparse Engine\n",
    "    - Other architectures: ResNet, EfficientNet, Inception\n",
    "- Use benchmarking scripts to compare performance across models and inference engines"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
